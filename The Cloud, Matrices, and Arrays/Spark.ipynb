{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from emrspark import *\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "conf.set(\"fs.s3n.awsAccessKeyId\",\"AKIAIV2VFIVWHJJP4OZA\")\n",
    "conf.set(\"fs.s3n.awsSecretAccessKey\",\"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\")\n",
    "spark = SparkSession.builder.config(conf=conf).appName('Graph HW3').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "a2q_sdf = spark.read.format(\"com.databricks.spark.csv\").option(\"delimiter\", ' ').load(\"s3n://upenn-bigdataanalytics/data/sx-stackoverflow-a2q.txt\")\n",
    "#c2q_sdf = spark.read.format(\"com.databricks.spark.csv\").option(\"delimiter\", ' ').load(\"s3n://upenn-bigdataanalytics/data/sx-stackoverflow-c2q.txt\")\n",
    "#c2a_sdf = spark.read.format(\"com.databricks.spark.csv\").option(\"delimiter\", ' ').load(\"s3n://upenn-bigdataanalytics/data/sx-stackoverflow-c2a.txt\")\n",
    "\n",
    "graph_sdf=a2q_sdf.selectExpr(\"_c0 as from_node\", \"_c1 as to_node\")\n",
    "#graph_sdf=graph_sdf.union(c2q_sdf.selectExpr(\"_c0 as from_node\", \"_c1 as to_node\"))\n",
    "#graph_sdf=graph_sdf.union(c2a_sdf.selectExpr(\"_c0 as from_node\", \"_c1 as to_node\"))\n",
    "\n",
    "graph_sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def transitive_closure(graph_sdf, origins_sdf, max_depth):\n",
    "    schema = StructType([StructField(\"node\", IntegerType(), True)])\n",
    "    frontier = origins_sdf\n",
    "    visited = sqlContext.createDataFrame(sc.emptyRDD(), schema)\n",
    "    graph_sdf.repartition(100, 'from_node')\n",
    "    graph_sdf.cache()\n",
    "    for i in range(max_depth):\n",
    "        next_nodes=get_nodes(graph_sdf,frontier, visited)\n",
    "        if i>0:\n",
    "            visited=visited.unionAll(frontier)\n",
    "        frontier.cache()\n",
    "        visited.cache()\n",
    "        frontier=next_nodes\n",
    "    visited=visited.unionAll(next_nodes)\n",
    "    return visited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_nodes(G,curr_nodes, visited):\n",
    "    \n",
    "    G.createOrReplaceTempView('graph_sdf_view')\n",
    "    curr_nodes.createOrReplaceTempView('frontier_sdf_view')\n",
    "    visited.createOrReplaceTempView('visited_sdf_view')\n",
    "    \n",
    "    connected_nodes=('SELECT DISTINCT g.to_node as node FROM graph_sdf_view AS g INNER JOIN frontier_sdf_view AS f ON f.node=g.from_node')\n",
    "    next_nodes=spark.sql(connected_nodes)\n",
    "    next_nodes.createOrReplaceTempView('next_nodes_sdf_view')\n",
    "    \n",
    "    remove_frontier=('SELECT DISTINCT n.node FROM next_nodes_sdf_view n LEFT JOIN frontier_sdf_view v ON v.node=n.node WHERE v.node IS NULL')\n",
    "    next_nodes=spark.sql(remove_frontier)\n",
    "    next_nodes.createOrReplaceTempView('next_nodes_sdf_view')\n",
    "    remove_visited=('SELECT DISTINCT n.node FROM next_nodes_sdf_view n LEFT JOIN visited_sdf_view v ON v.node=n.node WHERE v.node IS NULL')\n",
    "    hop_nodes=spark.sql(remove_visited)\n",
    "    return hop_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nodes_sdf=graph_sdf.filter(graph_sdf.from_node < 8).selectExpr('from_node as node')\n",
    "nodes_sdf=nodes_sdf.distinct()\n",
    "#nodes_sdf.show()\n",
    "\n",
    "reachable_sdf=transitive_closure(graph_sdf, nodes_sdf, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "541183"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reachable_sdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   node|\n",
      "+-------+\n",
      "|    296|\n",
      "|  13442|\n",
      "|    691|\n",
      "|  23113|\n",
      "|1018842|\n",
      "|  49280|\n",
      "|     51|\n",
      "| 254166|\n",
      "| 408870|\n",
      "|1018217|\n",
      "| 864197|\n",
      "|4328862|\n",
      "|  21238|\n",
      "| 982041|\n",
      "|   4782|\n",
      "|  31766|\n",
      "|  33690|\n",
      "|2951097|\n",
      "|   2144|\n",
      "|   5079|\n",
      "+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reachable_sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
